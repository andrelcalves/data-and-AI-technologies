A data integration pipeline is typically composed of an extract component, a transform component, and a contextualization component.

So, again A data integration pipeline is typically composed of an:
- extract component;
- transform component;
- contextualization component;

In This course youâ€™ll learn about the transform component that shapes and moves the data from the staging area into the CDF data model.

Important points
CDF Transformations is built on top of Apache Spark, an open-source big data processing engine. 

Note: To read our blog on CLI Transformations, [click here!](https://hub.cognite.com/developer-community-134/transformations-cli-726)

IMPORTANT: No run extractor as functiont in cognite because the cluster has limited resource.

Go to [hand-on](/01. Data Platform\1.1 Cognite Data Fusion (CDF)\1.1.8 Data Engineer Basic - Transform and Contextualize\1.1.8.1 CDF Transformations\hands-on.md)