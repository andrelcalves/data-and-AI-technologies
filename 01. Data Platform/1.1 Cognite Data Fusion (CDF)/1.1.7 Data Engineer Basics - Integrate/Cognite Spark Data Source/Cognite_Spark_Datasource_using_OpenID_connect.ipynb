{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "See also the [documentation](https://github.com/cognitedata/cdp-spark-datasource#reading-and-writing-cognite-data-platform-resource-types) for examples for each resource type. The general pattern is:\n",
        "\n",
        "```scala\n",
        "my_data_frame = spark.read.format(\"cognite.spark.v1\") \\\n",
        "  .option(\"type\", \"some-resource-type\") \\\n",
        "  .option(\"clientSecret\", dbutils.secrets.get(\"your-scope\", \"client-secret-for-project\"))\n",
        "```\n",
        "The resource types are:\n",
        "- `assets`\n",
        "- `events`\n",
        "- `timeseries` time series metadata\n",
        "- `datapoints` data points for a time series, also supports aggregates\n",
        "- `raw` \"RAW\" tables, which also require `.option(\"database\", \"some-database\")` and `.option(\"table\", \"some-table\")`\n",
        "\n",
        "Let's start by reading some data from the `publicdata` project."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "da266088-2747-4ba4-9bb9-89778cde102d"
        },
        "id": "eHubvTFOtbdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "secret_scope = \"\" # name your secret scope here\n",
        "project_key = \"\" # name the key to use from secret scope here"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "9e7ba207-aaad-46f9-8385-d77f48b01df7"
        },
        "id": "HsQRVcPMtbdd",
        "outputId": "cb0cef97-78c3-423f-d8ee-5f9acd480f5f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "assets = spark.read.format(\"cognite.spark.v1\") \\\n",
        "    .option(\"tokenUri\", \"https://login.microsoftonline.com/48d5043c-cf70-4c49-881c-c638f5796997/oauth2/v2.0/token\") \\\n",
        "    .option(\"clientId\", \"1b90ede3-271e-401b-81a0-a4d52bea3273\") \\\n",
        "    .option(\"clientSecret\", dbutils.secrets.get(secret_scope, project_key)) \\\n",
        "    .option(\"project\", \"publicdata\") \\\n",
        "    .option(\"scopes\", \"https://api.cognitedata.com/.default\") \\\n",
        "    .option(\"baseUrl\", \"https://api.cognitedata.com\") \\\n",
        "    .option(\"type\", \"assets\") \\\n",
        "    .load()  "
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "26a32b06-d81d-4e9c-b030-e4abe8ee4ac6"
        },
        "id": "wexegaxstbdf",
        "outputId": "8db636ee-7e51-4d9b-84cc-4997fc8d3ec3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataFrames\n",
        "\n",
        "We get back a [Spark DataFrame](https://spark.apache.org/docs/latest/sql-programming-guide.html) from `spark.read.format...load()`, which \"is conceptually equivalent to a table in a relational database or a data frame in R/Python\".\n",
        "\n",
        "`spark` is your entry point to the Spark API, and it's a `SparkSession` with a connection to the cluster. You will mostly use it to read data frames, and then interact with the data frames.\n",
        "\n",
        "You may have noticed that the command finished almost immediately. The data frame is a lazy data structure, and doesn't actually load any data until it has to. You can view the schema (the column names and their types) by clicking the small arrow next to the output. The schema is constant for assets, so we didn't need to read any data to produce that schema.\n",
        "\n",
        "Data is loaded only when you perform an *action* on a data frame, which requires data to be present. Examples of actions include `.count()` (for counting the rows), `.show()` (for printing the first few rows), `.toPandas()` (for converting to a Pandas data frame, downloading all data to your Python process), and pretty much anything else that uses data."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "d6b976a7-b023-4c28-aafb-9b22b316d9ed"
        },
        "id": "eoF44oCntbdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(assets.count())\n",
        "assets.show()\n",
        "assets.toPandas()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "cc3fd77d-d6aa-4ce9-88be-3b1e3f45aab9"
        },
        "id": "L33egBqQtbdg",
        "outputId": "f66ed4fb-44be-4f93-deab-0bf106c0a068"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "DataFrames can be distributed with many partitions being placed on different nodes in our Spark cluster."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "930d9038-9b20-47aa-a936-3c299c7fd2e8"
        },
        "id": "1YrXXVcrtbdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assets.rdd.getNumPartitions()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "8c88d8b3-99fd-43e1-a60f-f7fb302fb95a"
        },
        "id": "pNP5RZx1tbdh",
        "outputId": "c6bde080-6241-4053-e421-94b67b0d8057"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can drop and rename columns, getting a DataFrame with a new schema."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "eab1a8e1-da9e-475d-8290-245c0e98ec11"
        },
        "id": "piZ833IDtbdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assets.drop(\"metadata\") \\\n",
        "  .drop(\"externalId\") \\\n",
        "  .drop(\"source\") \\\n",
        "  .withColumnRenamed(\"description\", \"descr\") \\\n",
        "  .withColumnRenamed(\"lastUpdatedTime\", \"updatedAt\") \\\n",
        "  .printSchema()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "55051085-503f-426e-938e-004e60559afa"
        },
        "id": "QgowRBZ_tbdi",
        "outputId": "a3c2f53f-8c85-40d9-e60c-9ec5216bc6a4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notebooks have autocompletion built in and you can view keyboard shortcuts by clicking the keyboard icon at the top bar."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "792e8fe6-d2a0-4bc9-8085-401dbfdc4c6f"
        },
        "id": "MFgO432Otbdj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Displaying data\n",
        "\n",
        "In Databricks there's a convenient `display()` method you can use to show data in data frames (and a few other formats, like pandas and matplotlib figures). Since showing the data requires it to be loaded, this will also trigger an action.\n",
        "By default, only the first 1000 rows are displayed in the widget, even if Spark needs to load more data than this in the background.\n",
        "\n",
        "Note that you might need to scoll within the widget to show all the results.\n",
        "\n",
        "You can sort the rows shown by different columns, and you can expand the \"string to string\" map in the metadata column by clicking the arrow."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "cb0a66dc-b137-42a3-b561-0e9513e9afb0"
        },
        "id": "DvbLk0Jitbdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(assets)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "25d64da2-b27c-4492-b62b-74abd67523c6"
        },
        "id": "aoKnPqv0tbdk",
        "outputId": "36fb2462-e0a5-422f-f26a-2d58f6a79f82"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# we will explain groupBy() and count() in the section on aggregations\n",
        "display(assets.groupBy(assets.parentId).count())"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "fd148103-6f72-48ca-b96e-92f24ec923a6"
        },
        "id": "vrnuT5Ejtbdk",
        "outputId": "a1eac29e-9249-40f6-b5b1-937f3a3aa8de"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Caching data\n",
        "\n",
        "As we mentioned before, the data frame is a lazy structure that loads data when it is needed. Loading data over and over again can be slow and wasteful when we don't absolutely need it to be completely up-to-date.\n",
        "In that case, we can created a cached data frame by adding `.cache()` at the end."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b95abf89-3c7a-4394-a8bb-cd85744c8533"
        },
        "id": "6KnxU6ibtbdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "events = spark.read.format(\"cognite.spark.v1\") \\\n",
        "    .option(\"tokenUri\", \"https://login.microsoftonline.com/48d5043c-cf70-4c49-881c-c638f5796997/oauth2/v2.0/token\") \\\n",
        "    .option(\"clientId\", \"1b90ede3-271e-401b-81a0-a4d52bea3273\") \\\n",
        "    .option(\"clientSecret\", dbutils.secrets.get(secret_scope, project_key)) \\\n",
        "    .option(\"project\", \"publicdata\") \\\n",
        "    .option(\"scopes\", \"https://api.cognitedata.com/.default\") \\\n",
        "    .option(\"baseUrl\", \"https://api.cognitedata.com\") \\\n",
        "    .option(\"type\", \"assets\") \\\n",
        "    .load() \\\n",
        "    .cache()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "8ee6588a-fe94-4b57-97b5-7c28cfb6e2e0"
        },
        "id": "pfqTGNd4tbdl",
        "outputId": "a5f24168-1a7a-4571-e208-b32c4b02efa9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "events.printSchema()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "3b6c484a-2b48-41d2-bb3f-18cefc9413f0"
        },
        "id": "8n5rWpectbdl",
        "outputId": "23af8f2e-028c-4996-a63d-e112fd538358"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(events.count())\n",
        "events.show()\n",
        "events.toPandas()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "44b6d920-33d0-4852-a39a-5b856fcfac3f"
        },
        "id": "Lp5okx-Dtbdm",
        "outputId": "11ec4799-8a1f-417e-c906-083f67e50032"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we run the same commands again they should finish more quickly (potentially much more quickly if there's a lot of data)."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "98da16e9-dea1-49b6-ab6f-1958a1b18412"
        },
        "id": "raFpzj9vtbdm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(events.count())\n",
        "events.show()\n",
        "events.toPandas()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "07d820ba-4af6-4494-9a72-f19a8648f55d"
        },
        "id": "5inKjOrLtbdm",
        "outputId": "32584af4-f36f-4cbd-c5d0-0f189f533967"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "All asset data is now kept in memory by Spark, if possible, and reloads will happen only if a node crashes. Even then, only the data that was kept on that node will be reloaded, if possible.\n",
        "\n",
        "Caching is a good idea if you have a large amount of data that will not be changed.\n",
        "\n",
        "However, if you cache events as above, your cached copy will not receive new events.\n",
        "This might seem obvious, but it means that if you're doing something like waiting for new events that you\n",
        "have just created to show up, you should *not* cache the DataFrame you're using to check for new events!"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "6ffa868b-2b02-41a4-8274-dbcc5dff5308"
        },
        "id": "8YOTa2KDtbdn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Time series metadata\n",
        "\n",
        "Let's read the time series metadata into another cached data frame."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "06e335d7-0842-4051-8924-63bd0ea626b1"
        },
        "id": "Po1jQeW5tbdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tsmd = spark.read.format(\"cognite.spark.v1\") \\\n",
        "    .option(\"tokenUri\", \"https://login.microsoftonline.com/48d5043c-cf70-4c49-881c-c638f5796997/oauth2/v2.0/token\") \\\n",
        "    .option(\"clientId\", \"1b90ede3-271e-401b-81a0-a4d52bea3273\") \\\n",
        "    .option(\"clientSecret\", dbutils.secrets.get(secret_scope, project_key)) \\\n",
        "    .option(\"project\", \"publicdata\") \\\n",
        "    .option(\"scopes\", \"https://api.cognitedata.com/.default\") \\\n",
        "    .option(\"baseUrl\", \"https://api.cognitedata.com\") \\\n",
        "    .option(\"type\", \"timeseries\") \\\n",
        "    .load() \\\n",
        "    .cache()\n"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "8ee51525-bdab-47f0-a1d9-97315668fb77"
        },
        "id": "2pv5IkWztbdn",
        "outputId": "451dfa6a-01a9-4a95-9818-faba4df0ee4f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tsmd.printSchema()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "d7da90d8-6520-4628-a713-85469cbb9ce9"
        },
        "id": "UrjDtKnKtbdn",
        "outputId": "ada5e10a-2446-4612-ad7a-e8910e913a31"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tsmd.count()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "324d6061-f3ca-4f61-ae7f-811ff62424c1"
        },
        "id": "Asd44BSstbdo",
        "outputId": "11ac49db-0d26-4e2f-e565-3e3b5d2f262b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "display(tsmd)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "5d125e51-5fa0-4510-970b-12b2368436c8"
        },
        "id": "LXkwxmFAtbdo",
        "outputId": "715af71b-4082-4dd4-cbd6-452403c06583"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aggregations\n",
        "\n",
        "We can do things like group by and count using PySpark. For example, how many time series do we have per asset?\n",
        "One way to find out is to put time series metadata into different groups based on their asset id, and then count\n",
        "the number of items in each group, and then order the counts in a descending order."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b8bb69e3-4074-467e-b825-df0f4f1a755b"
        },
        "id": "84ZhwLBatbdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(tsmd.groupBy(\"assetId\").count().orderBy(\"count\", ascending=False))"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "0e798197-b9ee-4275-a4eb-03fca0cced1c"
        },
        "id": "7jiU2_9qtbdo",
        "outputId": "b0e1bd07-bd2e-49f2-8543-65d3c4119ce3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "How many different asset descriptions do we have, and how many assets per description?"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "bfda6974-75b4-4f4a-90be-0abf35e7efd2"
        },
        "id": "dbtjkZWKtbdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(assets.groupBy(\"description\").count().orderBy(\"count\", ascending=False))"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "0ccc952f-9fdd-489b-821c-f149e3d6e62f"
        },
        "id": "PPEoZRHOtbdp",
        "outputId": "da71acaf-dcf3-4bb3-aa94-8fe48bd18904"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark has support for many different [types of aggregations](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData), such as `min`, `max`, `mean`, `sum`, etc.\n",
        "\n",
        "We can make a plot of the number of time series associated per asset, to get an overall view of how many time series assets have in general.\n",
        "\n",
        "Since we have several \"counts\" in this query, we'll use `.withColumn` to rename the first one.\n",
        "We'll say more about `F` in a little bit, but for now we only need to know that `F.col(\"count\")` let's us refer to\n",
        "the column with the name \"count\"."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "d76e4927-1b87-4c49-b1c6-d42ddea6aa28"
        },
        "id": "PMELY3bStbdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "display(tsmd.groupBy(\"assetId\").count() \\\n",
        "        # We will do two \"count\" calls here, so we need to remember the counts per asset,\n",
        "        # by renaming the column named \"count\" at this point to \"countsPerAsset\"\n",
        "        .withColumn(\"countsPerAsset\", F.col(\"count\")) \\\n",
        "        .groupBy(\"countsPerAsset\") \\\n",
        "        # Now we count the number of assets with 1, 2, etc. time series connected to them.\n",
        "        .count() \\\n",
        "        # In order to avoid a random order in our bar chart we can sort by \"countsPerAsset\"\n",
        "        .orderBy(\"countsPerAsset\"))"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "df5ab4a9-4037-44e4-8f9f-ff983d172baa"
        },
        "id": "3xVBCX7jtbdp",
        "outputId": "cfd30f33-62b2-4cad-8611-34844df1f807"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we just want to know the average number of time series per asset, we can use `agg` and the `avg` function directly."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "bbf50d0a-c558-4d5b-8147-4f0991d602cf"
        },
        "id": "6G-PZgs8tbdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(tsmd.groupBy(\"assetId\").count().agg(F.avg(F.col(\"count\"))))"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "d5bb4a19-f3ac-4689-a327-6d36b03c24e8"
        },
        "id": "8gDWAUnStbdp",
        "outputId": "bf425bea-4680-4b80-f52d-cbb2556edc97"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will see more of `F` from here on, the `pyspark.sql.functions` package.\n",
        "Importing it as `F` allows us to use autocompletion to find functions in that package, and avoids\n",
        "ambiguities for functions like `min`, but it is also common to see individual methods imported like this:"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "73c7b761-fcc1-4515-8b99-c05b0f1a2aa5"
        },
        "id": "_fQwanletbdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import avg"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "76bca9b9-249f-4266-842a-bdb67f9e1cdc"
        },
        "id": "zvCWlvjvtbdq",
        "outputId": "3667d889-3899-4eda-9d60-08eb988ae1bb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since autocompletion is very useful, we recommend the `F` style."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "7fbb762a-ee21-45bf-8947-4b7b5d78fc19"
        },
        "id": "RckYiXE4tbdq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filtering\n",
        "\n",
        "We can use `.filter` or `.where` (same method by different names) to select a subset of data. `select` can be used to pick out specific columns, or even parts of columns like `metadata.SOURCE_TABLE`."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "fb66c21d-be92-4e4b-85f6-19237baf14d5"
        },
        "id": "bRKeXCBetbdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(assets.where(assets.description == \"VRD - 1ST STAGE COMPRESSOR LUBE OIL HEATER\") \\\n",
        "       .select(\"name\", \"description\", \"metadata.SOURCE_TABLE\"))"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "8824fe98-55b0-4faa-aff5-a1dcb7e25e9c"
        },
        "id": "zMySWJcgtbdq",
        "outputId": "4f54fcba-f7fb-4c4c-c909-a18498a40cb3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Root nodes are defined as having no parent, so their `.parentId` should be null."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "c66a5549-3335-42ef-86b5-fe02fc41e55e"
        },
        "id": "_TTetIuztbdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(assets.where(assets.parentId.isNull()))"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "6613bfa3-2222-409f-9d79-97c16d0aec4a"
        },
        "id": "l2R87kOztbdr",
        "outputId": "7ffc95b6-3504-46e8-f216-97344ef9a7e3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, we can look for uncontextualized time series metadata, which have a null `assetId`."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "c4e4e6cc-d3ce-4749-aeed-dbd615431fc5"
        },
        "id": "eZjW84MEtbdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(tsmd.where(tsmd.assetId.isNull()))"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "804dd4fe-c2f7-49ca-8bb5-cc845d3ebe91"
        },
        "id": "c_6HngeMtbdr",
        "outputId": "0715aeaa-6f1b-4bac-da14-9c5c0fbb187b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected, all time series in `publicdata` are contextualized. We can negate a filter expression using `~`\n",
        "to instead filter for time series that have been contextualized.\n",
        "\n",
        "In the case of filtering based on non-`NULL` values we can also use `.isNotNull()`."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "74e2ef65-6c83-413e-a544-c6681aa107be"
        },
        "id": "dtPmBvI1tbdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tsmd.where(~tsmd.assetId.isNull()).count())\n",
        "print(tsmd.where(tsmd.assetId.isNotNull()).count())"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "9961b93c-4223-450a-8b91-fa33e57ee12e"
        },
        "id": "ysAiFYLPtbdr",
        "outputId": "40795a7e-d79b-4ac3-8b1a-eba454156b06"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Column objects\n",
        "\n",
        "`assets.description` and `assets.parentId` return [Column](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.Column) objects.\n",
        "\n",
        "Column objects have a wide range of useful methods, and we will see many examples from here on out.\n",
        "We can also construct them from our DataFrame using string indexing, like `assets[\"description\"]`,\n",
        "which is necessary if the column name contains characters that are not valid Python identifiers.\n",
        "\n",
        "For example, we can say `assets[\"VALUE (%C)\"]`. We can also use `F.col(\"VALUE (%C)\")` to create a Column directly.\n",
        "However, if we do `F.col(\"name\")` and there are several DataFrames involved that have a `name` column, we'd be in trouble\n",
        "since we didn't specify which `name` column we meant, while `assets.name` would have been unambiguous.\n",
        "\n",
        "We'll see more of that when looking at joins.\n",
        "\n",
        "For those reasons, we recommend indexing the DataFrame (using `.name` when possible) to create Column objects, even if it can become a bit tedious to spell out the DataFrame name.\n",
        "\n",
        "However, in the previous section we use `F.col(\"count\")` because we didn't have a DataFrame object with a column\n",
        "named count. Our \"count\" column only existed on an intermediate DataFrame. We could have stored that DataFrame and\n",
        "given it a name, and then we could have used `df.count`, but sometimes it just makes sense to not bother naming each\n",
        "intermediate DataFrame."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "f0618401-09bb-4032-bb98-550cc5e9f933"
        },
        "id": "Pqd5x_vltbdr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Joins\n",
        "\n",
        "We can join data from different data frames together to answer questions like, what are the time series for asset ids `4050790831683279` and `3195126756929465`?"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "7948cac1-03aa-4d40-891e-3e615447d513"
        },
        "id": "iUq00jA1tbdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(assets.where(assets.id.isin([4050790831683279, 3195126756929465])) \\\n",
        "        .join(tsmd, tsmd.assetId == assets.id) \\\n",
        "        .select(assets.name, assets.description, tsmd.description, tsmd.name))"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "99b01a87-2bf7-4012-aad1-7f1b906dbbef"
        },
        "id": "P3MGXyLQtbds",
        "outputId": "c3093afd-2fc0-45d7-bf1a-26e22fd900a6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "When doing joins we often have the same column name in both tables, which can cause confusing results. As you can see, we ended up with two `description` columns and two `name` columns.\n",
        "\n",
        "`.alias` can be used to rename columns and help us keep track of which description belongs to the asset and which one belongs to the time series."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "d1df8bda-c682-4d9e-b9c2-a9118428a746"
        },
        "id": "wNsaLbaLtbds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(assets.where(assets.id.isin([4050790831683279, 3195126756929465]))\n",
        "        .join(tsmd, tsmd.assetId == assets.id)\n",
        "        .select(assets.name, assets.description, tsmd.description.alias(\"tsDescription\"), tsmd.name.alias(\"tsName\")))"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "acd834c2-6a4f-4bb7-8e51-be7480bc1bd8"
        },
        "id": "PGn2ujzYtbds",
        "outputId": "4c856569-7fed-44ce-d8b6-7c531dda6bbc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data points\n",
        "\n",
        "We can retrieve the data for a time series by using the `datapoints` resource type. This one is a bit special, because it will return no data unless you have specified the name(s) of the time series you want to get data for.\n",
        "\n",
        "As a consequence, you should *not* cache data frames using the `datapoints` resource type, otherwise the data frame will cache an empty result (and remain empty!) if you don't specify a time series name when querying it."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "2794666c-f3c4-48c7-a0d8-931bbe9e7a2d"
        },
        "id": "KIbqIqoPtbds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dp = spark.read.format(\"cognite.spark.v1\") \\\n",
        "    .option(\"tokenUri\", \"https://login.microsoftonline.com/48d5043c-cf70-4c49-881c-c638f5796997/oauth2/v2.0/token\") \\\n",
        "    .option(\"clientId\", \"1b90ede3-271e-401b-81a0-a4d52bea3273\") \\\n",
        "    .option(\"clientSecret\", dbutils.secrets.get(secret_scope, project_key)) \\\n",
        "    .option(\"project\", \"publicdata\") \\\n",
        "    .option(\"scopes\", \"https://api.cognitedata.com/.default\") \\\n",
        "    .option(\"baseUrl\", \"https://api.cognitedata.com\") \\\n",
        "    .option(\"type\", \"datapoints\") \\\n",
        "    .load() "
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "7ed15e61-4788-4db7-9ced-1ea61a51f6d7"
        },
        "id": "9atPUVFFtbds",
        "outputId": "31a7dc28-bebb-42f8-a0ab-ec78abe3e5a1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "display(dp.where(dp.externalId == \"pi:160184\") \\\n",
        "        .where(dp.timestamp > F.lit(\"2017-10-01\")) \\\n",
        "        .where(dp.timestamp < F.lit(\"2017-10-31\")))"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b37b8522-8937-49ed-a0f4-2a0d66604331"
        },
        "id": "4MgaPDqytbdt",
        "outputId": "1eae6264-ebf5-4934-fe5f-27a9f3efca43"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we don't specify an upper bound, [getLatest](https://doc.cognitedata.com/api/0.5/#operation/getLatest) will be\n",
        "used to retrieve the maximum timestamp available.\n",
        "\n",
        "Similarly, if there is no lower bound the Spark data source will make a query to the time series API to find the timestamp\n",
        "of the first available data point."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "7eb42dd0-7be5-492c-bfba-6584ca74161a"
        },
        "id": "0_Ru5SZbtbdt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Raw data points are downloaded by default, but the data points DataFrame also has full support for aggregates."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "a021fdd4-0d9a-4c4e-8742-50262c03c8b9"
        },
        "id": "yrDCQulitbdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(dp.where(dp.externalId == \"pi:160184\") \\\n",
        "        .where(dp.granularity == \"7d\") \\\n",
        "        .where(dp.aggregation.isin([\"min\", \"average\", \"max\"]))\n",
        "        .where(dp.timestamp > F.lit(\"2017-10-01\")) \\\n",
        "        .where(dp.timestamp < F.lit(\"2017-10-31\")))"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "2e7392ed-c342-4f5c-99ea-b8ccc9b34750"
        },
        "id": "at6B2AEntbdt",
        "outputId": "9840ae09-3c53-44be-be55-81687e91bb48"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting data\n",
        "\n",
        "The `display()` widget has a number of options for showing data in different ways, including a line plot that can group results by a column.\n",
        "\n",
        "Using this we can easily create a plot showing the minimum, average, and maximum values for a time series."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "264c5805-4b09-4738-ae88-11c5676856d0"
        },
        "id": "61YcstIitbdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(dp.where(dp.externalId == \"pi:160184\") \\\n",
        "        .where(dp.granularity == \"1d\") \\\n",
        "        .where(dp.aggregation.isin([\"min\", \"average\", \"max\"]))\n",
        "        .where(dp.timestamp > F.lit(\"2017-10-01\")) \\\n",
        "        .where(dp.timestamp < F.lit(\"2017-10-31\")))"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "c78cdf1f-89e9-4f96-9202-6f23253d9372"
        },
        "id": "IrYcOrwctbdt",
        "outputId": "21276fa7-6c34-4efc-bbb9-855f686986ab"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Joins with data points\n",
        "\n",
        "Due to limitations in Spark (that we may perhaps one day be able to work around) it's not possible to join `datapoints` directly, but we can get the externalIds of the time series we want to look at as a Python list by using `.collect()`.\n",
        "\n",
        "For example, let's say we want to look at data points from the time series with description `PH 1stStgComp Discharge` that are connected to the assets with description `VRD - PH 1STSTGCOMP DISCHARGE` that we found above. First we get the externalIds of those time series into a Python list."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "d9a3d076-db64-4b03-afd7-c6c601d84678"
        },
        "id": "cT2EUKQJtbdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "discharge_time_series = assets.where(assets.description == \"VRD - PH 1STSTGCOMP DISCHARGE\") \\\n",
        "  .join(tsmd, tsmd.assetId == assets.id) \\\n",
        "  .select(tsmd.externalId.alias(\"tsName\"))\n",
        "discharge_time_series_names = [ t.tsName for t in discharge_time_series.collect() ]\n",
        "discharge_time_series_names"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "4f370879-33a8-4400-a26a-ddf1db6c2b5d"
        },
        "id": "0frfOVcftbdu",
        "outputId": "36e44ac0-dd0f-4fb1-891c-ba781e165678"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we can use `.where(dp.name.isin(discharge_time_series_names))` to do the join we wanted."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "e2359b82-ecee-4aa4-ad72-63833df9b5ee"
        },
        "id": "iR5CUSRbtbdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(dp.where(dp.externalId.isin(discharge_time_series_names)) \\\n",
        "        .where(dp.timestamp > F.lit(\"2017-10-01\")) \\\n",
        "        .where(dp.aggregation == 'min') \\\n",
        "        .where(dp.granularity == \"7d\"))"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "24b55082-2922-4f83-8b7f-76eaef0bec5d"
        },
        "id": "oCjLl9XOtbdu",
        "outputId": "7c62a459-0b06-4b74-8ce5-1868307aa504"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Files metadata\n",
        "\n",
        "We also have support for files metadata. Currently we support reading and updating existing files metadata."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "d6e762c0-93c0-4233-9ebc-85a9dd491b38"
        },
        "id": "i-a2qrA_tbdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files = spark.read.format(\"cognite.spark.v1\") \\\n",
        "    .option(\"tokenUri\", \"https://login.microsoftonline.com/48d5043c-cf70-4c49-881c-c638f5796997/oauth2/v2.0/token\") \\\n",
        "    .option(\"clientId\", \"1b90ede3-271e-401b-81a0-a4d52bea3273\") \\\n",
        "    .option(\"clientSecret\", dbutils.secrets.get(secret_scope, project_key)) \\\n",
        "    .option(\"project\", \"publicdata\") \\\n",
        "    .option(\"scopes\", \"https://api.cognitedata.com/.default\") \\\n",
        "    .option(\"baseUrl\", \"https://api.cognitedata.com\") \\\n",
        "    .option(\"type\", \"files\") \\\n",
        "    .load()  "
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "ec8b31ab-10f6-4b02-adad-57bed999d4b8"
        },
        "id": "9uWjeC2ctbdv",
        "outputId": "4ad93ea3-9e43-40de-d93b-6a9e6fa86d86"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "files.printSchema()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "da0a82aa-d6b5-4002-993f-4766466c7c1e"
        },
        "id": "_0eSALu7tbdv",
        "outputId": "8a624311-30bb-4783-fbe2-77eb4d0713d7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "display(files.groupBy(files.mimeType) \\\n",
        "        .count() \\\n",
        "        .orderBy(\"count\", ascending=False))"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "c5daa44f-cf66-4fd0-b4f3-fc5130c369ca"
        },
        "id": "3niUjC3Vtbdv",
        "outputId": "f0a418f2-dfbd-4429-dda0-a7f571995705"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "4eea9e6c-8e2e-4d2c-9fef-5e56cc0d32b0"
        },
        "id": "3AaEQbrztbdv",
        "outputId": "9bd7a08d-9ed0-4d61-fe27-8cdb4f473ba8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    }
  ],
  "metadata": {
    "language_info": {
      "mimetype": "text/x-python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "version": "3.7.6",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "name": "Tutorial",
    "notebookId": 1572025699111510,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "application/vnd.databricks.v1+notebook": {
      "notebookName": "Cognite Spark Datasource using Databricks Community",
      "dashboards": [],
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "language": "python",
      "widgets": {},
      "notebookOrigID": 967342071265178
    },
    "colab": {
      "name": "Cognite Spark Datasource using OpenID connect.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Po1jQeW5tbdn",
        "84ZhwLBatbdo",
        "bRKeXCBetbdq",
        "Pqd5x_vltbdr",
        "iUq00jA1tbdr",
        "61YcstIitbdt",
        "cT2EUKQJtbdu",
        "i-a2qrA_tbdu"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}